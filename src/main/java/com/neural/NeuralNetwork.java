package com.neural;
/*
 * This Java source file was auto generated by running 'gradle buildInit --type java-library'
 * by 'colin_000' at '09/03/18 8:44 PM' with Gradle 2.14.1
 *
 * @author colin_000, @date 09/03/18 8:44 PM
 */

import Jama.Matrix;

public class NeuralNetwork {
	
	public enum ACTIVATION_FUNC {
		SIGMOID,
		RELU,
		TANH,
		SOFTMAX,
		NONE
	};
	
	public enum WEIGHT_INIT_FUNC {
		RANDOM,
		XAVIER_MODIFIED
	};
	
	public enum PROBLEM_TYPE {
		REGRESSION,
		CLASSIFICATION
	};
	
	private PROBLEM_TYPE problemType = PROBLEM_TYPE.REGRESSION;
		
	private Layer[] layersNodesWeightsBias;
	
	public NeuralNetwork(int numNodesInLayers[], 
						double _learningRate, 
						double _momentumFactor, 
						ACTIVATION_FUNC activationFuncs[], 
						WEIGHT_INIT_FUNC _weightInitFunc) {
		Init(numNodesInLayers, activationFuncs, _learningRate, _momentumFactor, _weightInitFunc);
    }
	
	public NeuralNetwork(int numNodesInLayers[], 
						double _learningRate, 
						double _momentumFactor, 
						ACTIVATION_FUNC _activationFunc, 
						WEIGHT_INIT_FUNC _weightInitFunc) {
		ACTIVATION_FUNC activationFuncs[] = new ACTIVATION_FUNC[numNodesInLayers.length];
		
		for(int i = 0; i < activationFuncs.length; ++i)
		{
			activationFuncs[i] = _activationFunc;
		}
		
		Init(numNodesInLayers, activationFuncs, _learningRate, _momentumFactor, _weightInitFunc);
	}
	
	public void SetProblemType(PROBLEM_TYPE _problemType){
		problemType = _problemType;
	}
	
	private Layer CreateLayer(int inputNodes, int outputNodes, ACTIVATION_FUNC activationFunc, double _momentumFactor, double _learningRate, WEIGHT_INIT_FUNC _weightInitFunc){
		switch (activationFunc){
		case SOFTMAX:
			return new SoftmaxLayer(inputNodes, outputNodes, _momentumFactor, _learningRate, _weightInitFunc);
		case SIGMOID:
			return new SigmoidLayer(inputNodes, outputNodes, _momentumFactor, _learningRate, _weightInitFunc);
		case RELU:
			return new ReLULayer(inputNodes, outputNodes, _momentumFactor, _learningRate, _weightInitFunc);
		case TANH:
			return new TanHLayer(inputNodes, outputNodes, _momentumFactor, _learningRate, _weightInitFunc);
		case NONE:
			return new InputLayer(inputNodes, outputNodes, _momentumFactor, _learningRate, _weightInitFunc);
		}
		
		return null;
	}
	
	private void Init(int numNodesInLayers[], ACTIVATION_FUNC actFuncs[], double _momentumFactor, double _learningRate, WEIGHT_INIT_FUNC _weightInitFunc) {
		if(_weightInitFunc == null){
			_weightInitFunc = WEIGHT_INIT_FUNC.RANDOM;
		}
		
		if(actFuncs[actFuncs.length - 1] == ACTIVATION_FUNC.SOFTMAX){
			problemType = PROBLEM_TYPE.CLASSIFICATION;
		}
		
		actFuncs[0] = ACTIVATION_FUNC.NONE;
		
		layersNodesWeightsBias = new Layer[numNodesInLayers.length];
		
		// Add correct number of nodes, and then add weights and bias
		for(int i = 0; i < numNodesInLayers.length; ++i)
		{
			// Create correct number of nodes
			if(i == 0){
				layersNodesWeightsBias[i] = CreateLayer(numNodesInLayers[i], numNodesInLayers[i], actFuncs[i], _momentumFactor, _learningRate, _weightInitFunc);
			} else {
				layersNodesWeightsBias[i] = CreateLayer(numNodesInLayers[i - 1], numNodesInLayers[i], actFuncs[i], _momentumFactor, _learningRate, _weightInitFunc);
			}
		}
	}
	
	// Get inputs to the network, return outputs of the network
	public double[] goThroughNetwork(double inputs[], boolean trainingMode, double idealOutputs[])
	{
		int numOutputLayers = layersNodesWeightsBias.length;
		
		Matrix outputs[] = null;
		outputs = new Matrix[numOutputLayers];
		
		// Get the outputs of the first derivative of the activation functions
		Matrix outputsPrime[] = null;
		outputsPrime = new Matrix[numOutputLayers];
		
		// The inputs become a row vector
		double inputsTemp[][] = new double[1][];
		inputsTemp[0] = inputs;
		Matrix inputsVector = new Matrix(inputsTemp);
		
		//For each layer, multiply inputs by the weights + bias, then send outputs to next layer
		for(int layerNum = 0; layerNum < layersNodesWeightsBias.length; ++layerNum){
			Layer layer = layersNodesWeightsBias[layerNum];
			
			int layerLength = layer.GetLayerLength();
			
			outputs[layerNum] = new Matrix(layerLength, 1);
			outputsPrime[layerNum] = new Matrix(layerLength, 1);
			
			// For each node, multiple inputs by weights then add bias
			if(layerNum == 0){
				// If this is the input node, then match each input with a node
				outputs[layerNum] = layer.takeInput(inputsVector);
			} else {
				outputs[layerNum] = layer.takeInput(outputs[layerNum - 1]);
				outputsPrime[layerNum] = layer.takeInputPrime(outputs[layerNum - 1]);
			}
		}
		
		int outputLayerIndex = layersNodesWeightsBias.length - 1;
		
		if(trainingMode){
			doTraining(outputs, idealOutputs, outputsPrime, layersNodesWeightsBias);
		}
		
		return outputs[outputLayerIndex].getArrayCopy()[0];
	}
	
	private void doTraining(Matrix[] outputs, double[] idealOutputs, Matrix[] outputsPrime, Layer[] layers){
		
		int outputLayerIndex = layers.length - 1;
		
		Matrix[] layerErrors = new Matrix[layers.length];
		
		if(problemType == PROBLEM_TYPE.REGRESSION){
			// Calculate error vector of the output layer
			Matrix gradientVectorMaxtrix = gradientCostFuncMeanSquared(idealOutputs, outputs[outputLayerIndex]);
			
			// Printing the difference between the ideal outputs and the actual outputs
			// gradientVectorMaxtrix.print( 2, 5);

			layerErrors[outputLayerIndex] = (gradientVectorMaxtrix.arrayTimes(outputsPrime[outputLayerIndex])).transpose();
		} else {
			layerErrors[outputLayerIndex] = gradientCostFuncMeanSquared(idealOutputs, outputs[outputLayerIndex]).transpose();
		}
		
		//Backpropogate the error
		for(int i = outputLayerIndex - 1; i > 0; --i) {
			
			// Get the weights matrix, do w * err
			Matrix weightsMatrix = layers[i + 1].getWeightsMatrix();
			Matrix applied = weightsMatrix.transpose().times(layerErrors[i + 1]);
			
			// Finally get the error for the layer using outputs primes from during the training
			layerErrors[i] = applied.arrayTimes(outputsPrime[i].transpose());
		}
		
		// Now use the error terms and the outputs to determine the how weights and bias should be changed
		for(int i = outputLayerIndex; i > 0; --i) {
			layers[i].adjustWeightsAndBias(outputs[i - 1], layerErrors[i]);
		}
	}
	
	// Calculate gradient of cost with respect to output layer
	private Matrix gradientCostFuncMeanSquared(double[] idealOutputs, Matrix actualOutputs){
		
		Matrix gradientVector = new Matrix(1, idealOutputs.length);
		
		// Get the difference vector
		for(int i = 0; i < idealOutputs.length; ++i){
			// Quadratic Cost Function
			gradientVector.set(0, i, idealOutputs[i] - actualOutputs.get(0,i));
		}
		
		return gradientVector;
	}
}

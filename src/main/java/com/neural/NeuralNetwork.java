package com.neural;
/*
 * This Java source file was auto generated by running 'gradle buildInit --type java-library'
 * by 'colin_000' at '09/03/18 8:44 PM' with Gradle 2.14.1
 *
 * @author colin_000, @date 09/03/18 8:44 PM
 */

import Jama.Matrix;

public class NeuralNetwork {
	
	public enum ACTIVATION_FUNC {
		SIGMOID,
		RELU,
		TANH,
		NONE
	};
	
	public enum WEIGHT_INIT_FUNC {
		RANDOM,
		XAVIER_MODIFIED
	};
	
	public enum PROBLEM_TYPE {
		REGRESSION,
		CLASSIFICATION
	};
	
	private PROBLEM_TYPE problemType = PROBLEM_TYPE.REGRESSION;
		
	private Node[][] layersNodesWeightsBias;
	
	public NeuralNetwork(int numNodesInLayers[], 
						double _learningRate, 
						double _momentumFactor, 
						ACTIVATION_FUNC activationFuncs[], 
						WEIGHT_INIT_FUNC _weightInitFunc) {
		Init(numNodesInLayers, activationFuncs, _learningRate, _momentumFactor, _weightInitFunc);
    }
	
	public NeuralNetwork(int numNodesInLayers[], 
						double _learningRate, 
						double _momentumFactor, 
						ACTIVATION_FUNC _activationFunc, 
						WEIGHT_INIT_FUNC _weightInitFunc) {
		ACTIVATION_FUNC activationFuncs[] = new ACTIVATION_FUNC[numNodesInLayers.length];
		
		for(int i = 0; i < activationFuncs.length; ++i)
		{
			activationFuncs[i] = _activationFunc;
		}
		
		Init(numNodesInLayers, activationFuncs, _learningRate, _momentumFactor, _weightInitFunc);
	}
	
	public void SetProblemType(PROBLEM_TYPE _problemType){
		problemType = _problemType;
	}
	
	private void Init(int numNodesInLayers[], ACTIVATION_FUNC actFuncs[], double _learningRate, double _momentumFactor, WEIGHT_INIT_FUNC _weightInitFunc) {
		if(_weightInitFunc == null){
			_weightInitFunc = WEIGHT_INIT_FUNC.RANDOM;
		}
		
		layersNodesWeightsBias = new Node[numNodesInLayers.length][];
		
		// Add correct number of nodes, and then add weights and bias
		for(int i = 0; i < numNodesInLayers.length; ++i)
		{
			// Create correct number of nodes
			layersNodesWeightsBias[i] = new Node[numNodesInLayers[i]];
			
			// Initialize each node
			for(int k = 0; k < numNodesInLayers[i]; ++k)
			{
				if(i == 0){
					layersNodesWeightsBias[i][k] = new InputNode();
				} else {
					switch (actFuncs[i]){
					case SIGMOID:
						layersNodesWeightsBias[i][k] = new SigmoidNode(numNodesInLayers[i - 1], _momentumFactor, _learningRate, _weightInitFunc);
						break;
					case RELU:
						layersNodesWeightsBias[i][k] = new ReLUNode(numNodesInLayers[i - 1], _momentumFactor, _learningRate, _weightInitFunc);
						break;
					case TANH:
						layersNodesWeightsBias[i][k] = new TanHNode(numNodesInLayers[i - 1], _momentumFactor, _learningRate, _weightInitFunc);
						break;
					default:
						layersNodesWeightsBias[i][k] = new SigmoidNode(numNodesInLayers[i - 1], _momentumFactor, _learningRate, _weightInitFunc);
					}
				}
			}
		}
	}
	
	// Get inputs to the network, return outputs of the network
	public double[] goThroughNetwork(double inputs[], boolean trainingMode, double idealOutputs[])
	{
		int numOutputLayers = layersNodesWeightsBias.length;
		
		// Add a Softmax layer if this is a classification problem
		if(problemType == PROBLEM_TYPE.CLASSIFICATION){
			++numOutputLayers;
		}
		
		double outputs[][] = null;
		outputs = new double[numOutputLayers][];
		
		// Get the outputs of the first derivative of the Sigmoid functions
		double outputsPrime[][] = null;
		outputsPrime = new double[layersNodesWeightsBias.length][];
		
		//For each layer, multiply inputs by the weights + bias, then send outputs to next layer
		for(int layerNum = 0; layerNum < layersNodesWeightsBias.length; ++layerNum){
			Node[] layer = layersNodesWeightsBias[layerNum];
			
			outputs[layerNum] = new double[layer.length];
			outputsPrime[layerNum] = new double[layer.length];
			
			// For each node, multiple inputs by weights then add bias
			for(int nodeNum = 0; nodeNum < layer.length; ++nodeNum){
				
				// If this is the input node, then match each input with a node
				if(layerNum == 0){
					double singleInput[] = new double[1];
					singleInput[0] = inputs[nodeNum];
					
					outputs[layerNum][nodeNum] = layer[nodeNum].takeInput(singleInput);
				} else {
					outputs[layerNum][nodeNum] = layer[nodeNum].takeInput(outputs[layerNum - 1]);
					outputsPrime[layerNum][nodeNum] = layer[nodeNum].takeInputPrime(outputs[layerNum - 1]);
				}
			}
		}
		
		int outputLayerIndex = layersNodesWeightsBias.length - 1;
		
		// Calculate output of the Softmax layer if this is a classification problem
		if(problemType == PROBLEM_TYPE.CLASSIFICATION){
			
			double outputLayerSum = 0;
			
			// Get sum of outputs from second last layer
			for(int i = 0; i < outputs[outputLayerIndex].length; ++i){
				outputLayerSum += outputs[outputLayerIndex][i];
			}
			
			// Calculate output of softmax layer
			for(int i = 0; i < outputs[outputLayerIndex].length; ++i){
				outputs[outputLayerIndex + 1][i] = outputs[outputLayerIndex][i]/outputLayerSum;
			}
			
			++outputLayerIndex;
		}
		
		if(trainingMode){
			doTraining(outputs, idealOutputs, outputsPrime, layersNodesWeightsBias);
		}
		
		return outputs[outputLayerIndex];
	}
	
	private void doTraining(double[][] outputs, double[] idealOutputs, double[][] outputsPrime, Node[][] layersNodesWeightsBias){
		
		int outputLayerIndex = layersNodesWeightsBias.length - 1;
		
		Matrix[] layerErrors = new Matrix[layersNodesWeightsBias.length];
		
		// Calculate error vector of the output layer
		double[] temp = gradientCostFuncMeanSquared(idealOutputs, outputs[outputLayerIndex]);
		double[][] gradientVector = new double[1][];
		gradientVector[0] = temp;
		Matrix gradientVectorMaxtrix = new Matrix(gradientVector);
		
		// Printing the difference between the ideal outputs and the actual outputs
		// gradientVectorMaxtrix.print( 2, 5);
		
		double[][] outputPrimeVector = new double[1][];
		outputPrimeVector[0] = outputsPrime[outputLayerIndex];
		Matrix outputVectorMatrix = new Matrix(outputPrimeVector);
		
		layerErrors[outputLayerIndex] = gradientVectorMaxtrix.arrayTimes(outputVectorMatrix).transpose();
		
		//Backpropogate the error
		for(int i = outputLayerIndex - 1; i > 0; --i){
			
			// Get the weights matrix, do w * err
			Matrix weightsMatrix = constructWeightsMatrix(layersNodesWeightsBias[i + 1]);
			Matrix applied = weightsMatrix.transpose().times(layerErrors[i + 1]);
			
			// Just get the outputs of the sigmoid primes from during the training
			double[][] layerOutputVector = new double[1][];
			layerOutputVector[0] = outputsPrime[i];
			Matrix layerOutputVectorMatrix = new Matrix(layerOutputVector).transpose();
			
			// Finally get the error for the layer
			layerErrors[i] = applied.arrayTimes(layerOutputVectorMatrix);
		}
		
		// Now use the error terms and the outputs to determine the how weights and bias should be changed
		for(int i = outputLayerIndex; i > 0; --i){
			
			Matrix layerError = layerErrors[i];
			
			// Iterate over the nodes in a given layer
			for(int k = 0; k < layersNodesWeightsBias[i].length; ++k){
				Node node = layersNodesWeightsBias[i][k];
				
				Matrix weightDeltas = new Matrix(outputs[i - 1].length, 1);
				
				// Iterate over the outputs of the previous layer
				for(int j = 0; j < outputs[i - 1].length; ++j){
					double weightDelta = outputs[i - 1][j] * layerError.get(k, 0);
					weightDeltas.set(j, 0, weightDelta);
				}
				
				node.adjustWeights(weightDeltas);
				node.adjustBias(layerError.get(k, 0));
			}
		}
	}
	
	private Matrix constructWeightsMatrix(Node layer[]){
		double[][] weights = new double[layer.length][];
		
		for(int i = 0; i < layer.length; ++i) {
			weights[i] = layer[i].getWeights()[0];
		}
		
		Matrix ret = new Matrix(weights);
		
		return ret;
	}
	
	// Calculate gradient of cost with respect to output layer
	private double[] gradientCostFuncMeanSquared(double idealOutputs[], double actualOutputs[]){
		
		double gradientVector[] = new double[idealOutputs.length];
		
		// Get the difference vector
		for(int i = 0; i < idealOutputs.length; ++i){
			// Quadratic Cost Function
			gradientVector[i] = idealOutputs[i] - actualOutputs[i];
		}
		
		return gradientVector;
	}
	
	// Calculate gradient of cost with respect to output layer
	private double[] gradientCostFuncCrossEntropy(double idealOutputs[], double actualOutputs[]){
		
		double gradientVector[] = new double[idealOutputs.length];
		
		// Get the difference vector
		for(int i = 0; i < idealOutputs.length; ++i){
			// Cross Entropy Cost Function
			gradientVector[i] = actualOutputs[i] - idealOutputs[i];
		}
		
		return gradientVector;
	}
}
